{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d49349-faaa-420c-9b65-0bdc9edfabce",
   "metadata": {},
   "source": [
    "# The Price is Right\n",
    "\n",
    "Today we build a more complex solution for estimating prices of goods.\n",
    "\n",
    "1. Day 2.0 notebook: create a RAG database with our 400,000 training data\n",
    "2. Day 2.1 notebook: visualize in 2D\n",
    "3. Day 2.2 notebook: visualize in 3D\n",
    "4. Day 2.3 notebook: build and test a RAG pipeline with GPT-4o-mini\n",
    "5. Day 2.4 notebook: (a) bring back our Random Forest pricer (b) Create a Ensemble pricer that allows contributions from all the pricers\n",
    "\n",
    "Phew! That's a lot to get through in one day!\n",
    "\n",
    "## PLEASE NOTE:\n",
    "\n",
    "We already have a very powerful product estimator with our proprietary, fine-tuned LLM. Most people would be very satisfied with that! The main reason we're adding these extra steps is to deepen your expertise with RAG and with Agentic workflows.\n",
    "\n",
    "## Finishing off with Random Forests & Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcdfea8-7241-46d7-a771-c0381a3e7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import chromadb\n",
    "from items import Item\n",
    "from testing import Tester\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e88bd1-f89c-4b98-92fa-aa4bc1575bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "QUESTION = \"How much does this cost to the nearest dollar?\\n\\n\"\n",
    "DB = \"products_vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98666e73-938e-469d-8987-e6e55ba5e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc696493-0b6f-48aa-9fa8-b1ae0ecaf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the test pickle file:\n",
    "\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26a1104-cd11-4361-ab25-85fb576e0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=DB)\n",
    "collection = client.get_or_create_collection('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00b82a9-a8dc-46f1-8ea9-2f07cbc8e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "prices = [metadata['price'] for metadata in result['metadatas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6492cb-b11a-4ad5-859b-a71a78ffb949",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "We will now train a Random Forest model.\n",
    "\n",
    "Can you spot the difference from what we did in Week 6? In week 6 we used the word2vec model to form vectors; this time we'll use the vectors we already have in Chroma, from the SentenceTransformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48894777-101f-4fe5-998c-47079407f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next line takes an hour on my M1 Mac!\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(vectors, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb7ddf-e1da-481e-84c6-1256547566bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281dc5e-761e-4a5e-86b3-29d9c0a33d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMKAR\\anaconda3\\envs\\llms\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load it back in again\n",
    "\n",
    "rf_model = joblib.load('random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d438dec-8e5b-4e60-bb6f-c3f82e522dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.specialist_agent import SpecialistAgent\n",
    "from agents.frontier_agent import FrontierAgent\n",
    "from agents.random_forest_agent import RandomForestAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc39369-b97b-4a90-b17e-b20ef501d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMKAR\\anaconda3\\envs\\llms\\Lib\\asyncio\\events.py:84: DeprecationError: 2025-01-27: `modal.Cls.lookup` is deprecated and will be removed in a future release. It can be replaced with `modal.Cls.from_name`.\n",
      "\n",
      "See https://modal.com/docs/guide/modal-1-0-migration for more information.\n",
      "  self._context.run(self._callback, *self._args)\n",
      "C:\\Users\\OMKAR\\anaconda3\\envs\\llms\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.8 MiB for an array with shape (488035,) and data type {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m specialist = SpecialistAgent()\n\u001b[32m      2\u001b[39m frontier = FrontierAgent(collection)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m random_forest = \u001b[43mRandomForestAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\llm_engineering\\week8\\agents\\random_forest_agent.py:24\u001b[39m, in \u001b[36mRandomForestAgent.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mRandom Forest Agent is initializing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.vectorizer = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrandom_forest_model.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mRandom Forest Agent is ready\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode)\u001b[39m\n\u001b[32m    652\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    653\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    654\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    655\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[32m    656\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, filename, mmap_mode)\u001b[39m\n\u001b[32m    575\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    579\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m                       \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m                       \u001b[33m\"\u001b[39m\u001b[33mPlease regenerate this pickle file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m                       % filename,\n\u001b[32m    583\u001b[39m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\pickle.py:1213\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1211\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\joblib\\numpy_pickle.py:415\u001b[39m, in \u001b[36mNumpyUnpickler.load_build\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array_wrapper, NDArrayWrapper):\n\u001b[32m    414\u001b[39m     \u001b[38;5;28mself\u001b[39m.compat_mode = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m \u001b[38;5;28mself\u001b[39m.stack.append(\u001b[43marray_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\joblib\\numpy_pickle.py:252\u001b[39m, in \u001b[36mNumpyArrayWrapper.read\u001b[39m\u001b[34m(self, unpickler)\u001b[39m\n\u001b[32m    250\u001b[39m     array = \u001b[38;5;28mself\u001b[39m.read_mmap(unpickler)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpickler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Manage array subclass case\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(array, \u001b[33m'\u001b[39m\u001b[33m__array_prepare__\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mself\u001b[39m.subclass \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (unpickler.np.ndarray,\n\u001b[32m    257\u001b[39m                           unpickler.np.memmap)):\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# We need to reconstruct another subclass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\joblib\\numpy_pickle.py:173\u001b[39m, in \u001b[36mNumpyArrayWrapper.read_array\u001b[39m\u001b[34m(self, unpickler)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# crc32 module fails on reads greater than 2 ** 32 bytes,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# of the read. In non-chunked case count < max_read_count, so\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# only one read is performed.\u001b[39;00m\n\u001b[32m    170\u001b[39m max_read_count = BUFFER_SIZE // \u001b[38;5;28mmin\u001b[39m(BUFFER_SIZE,\n\u001b[32m    171\u001b[39m                                     \u001b[38;5;28mself\u001b[39m.dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m array = unpickler.np.empty(count, dtype=\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, count, max_read_count):\n\u001b[32m    175\u001b[39m     read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 29.8 MiB for an array with shape (488035,) and data type {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}"
     ]
    }
   ],
   "source": [
    "specialist = SpecialistAgent()\n",
    "frontier = FrontierAgent(collection)\n",
    "random_forest = RandomForestAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d0d0a-8bb8-4b39-b046-322828c39244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(item):\n",
    "    return item.prompt.split(\"to the nearest dollar?\\n\\n\")[1].split(\"\\n\\nPrice is $\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0434f-b29e-4cc0-bad9-b07624665727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(item):\n",
    "    return random_forest.price(description(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf233ec-264f-4b34-9f2b-27c39692137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(rf, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f759bd2-7a7e-4c1a-80a0-e12470feca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Quadcast HyperX condenser mic for high quality audio for podcasting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dbd25-fb95-4b6b-bbbb-8da5fc817105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(specialist.price(product))\n",
    "print(frontier.price(product))\n",
    "print(random_forest.price(product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779b353-e2bb-4fc7-be7c-93057e4d688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists = []\n",
    "frontiers = []\n",
    "random_forests = []\n",
    "prices = []\n",
    "for item in tqdm(test[1000:1250]):\n",
    "    text = description(item)\n",
    "    specialists.append(specialist.price(text))\n",
    "    frontiers.append(frontier.price(text))\n",
    "    random_forests.append(random_forest.price(text))\n",
    "    prices.append(item.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bca725-4e34-405b-8d90-41d67086a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = [min(s,f,r) for s,f,r in zip(specialists, frontiers, random_forests)]\n",
    "maxes = [max(s,f,r) for s,f,r in zip(specialists, frontiers, random_forests)]\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'Specialist': specialists,\n",
    "    'Frontier': frontiers,\n",
    "    'RandomForest': random_forests,\n",
    "    'Min': mins,\n",
    "    'Max': maxes,\n",
    "})\n",
    "\n",
    "# Convert y to a Series\n",
    "y = pd.Series(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5be8a-3e7f-42a2-be54-0c7e380f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Linear Regression\n",
    "np.random.seed(42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "feature_columns = X.columns.tolist()\n",
    "\n",
    "for feature, coef in zip(feature_columns, lr.coef_):\n",
    "    print(f\"{feature}: {coef:.2f}\")\n",
    "print(f\"Intercept={lr.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf6e68-28a3-4ed2-b17e-de0ede923d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr, 'ensemble_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762441a-9470-4dd7-8a8f-ec0430e908c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ensemble_agent import EnsembleAgent\n",
    "ensemble = EnsembleAgent(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29f03c-8010-43b7-ae7d-1bc85ca6e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.price(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5e226-a508-43d5-aa42-cefbde72ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_pricer(item):\n",
    "    return max(0,ensemble.price(description(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397b1ef-2ea3-4af8-bb34-36594e0600cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(ensemble_pricer, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c5350-d4b5-42ae-96f6-ec94f6ab41d7",
   "metadata": {},
   "source": [
    "# WHAT A DAY!\n",
    "\n",
    "We got so much done - a Fronter RAG pipeline, a Random Forest model using transformer-based encodings, and an Ensemble model.\n",
    "\n",
    "You can do better, for sure!\n",
    "\n",
    "Tweak this, and try adding components into the ensemble, to beat my performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85009065-851e-44a2-b39f-4c116f7fbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
